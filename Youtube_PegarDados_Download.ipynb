{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from pytube import YouTube\n",
    "\n",
    "\n",
    "#url de vídeos do BernoulliTV pronto para busca, bastando acrescentar o código no final do link\n",
    "URL_BUSCA_BPLAY = 'https://www.youtube.com/channel/UClsm8e0m-a-pLQTE31YmasQ/search?query='\n",
    "\n",
    "with open('YoutubeAPI.txt', 'r') as f: # Lê a chave da API do Youtube\n",
    "    your_api_key = f.read()\n",
    "\n",
    "def dados_com_link(link): # Pega os dados de Likes, Dislikes, Visualizações e Data de postagem para o 'link'\n",
    "    if link.count('=') == 1:\n",
    "        id_of_video = link[link.find('=')+1:]\n",
    "        url = 'https://www.youtube.com/watch?v=' + id_of_video\n",
    "    if link.count('=') == 2:\n",
    "        id_of_video = link[link.find('=')+1:link.rfind('=')]\n",
    "        url = 'https://www.youtube.com/watch?v=' + id_of_video\n",
    "    url_youtube_estatistica = f'https://www.googleapis.com/youtube/v3/videos?id={id_of_video}&key={your_api_key}&part=statistics'\n",
    "    url_youtube_snippet = f'https://www.googleapis.com/youtube/v3/videos?id={id_of_video}&key={your_api_key}&part=snippet'\n",
    "    json_estatistica = requests.get(url_youtube_estatistica).json()['items'][0]['statistics']\n",
    "    json_snippet = requests.get(url_youtube_snippet).json()['items'][0]['snippet']\n",
    "    visualizacoes = json_estatistica['viewCount']\n",
    "    likes = json_estatistica['likeCount']\n",
    "    dislikes = json_estatistica['dislikeCount']\n",
    "    data_postagem = json_snippet['publishedAt'].split('T')[0]\n",
    "    return {'Link':[url], 'Visualizações': [int(visualizacoes)], 'Likes': [int(likes)], 'Dislikes':[int(dislikes)], 'Postagem':[data_postagem]}\n",
    "\n",
    "    \n",
    "def link_com_codigo_BPlay(cod): # Usa a url de busca do BernoulliPlay para pegar o link de visualização do código 'cod'\n",
    "    source = requests.get(URL_BUSCA_BPLAY + cod).text\n",
    "    soup = BeautifulSoup(source, \"html.parser\")\n",
    "    for link in soup.select('a[href^=\"/watch\"]'):\n",
    "        if cod.upper() in link.text.upper():\n",
    "            parte_video = link.get('href')\n",
    "            print(parte_video)\n",
    "            return ('https://www.youtube.com' + parte_video)\n",
    "\n",
    "        \n",
    "# Pega os dados (inclusive o link) de todos os vídeos pelos códigos na lista_cod, acumula num DataFrame e salva o excel compilado\n",
    "def script_dados_codigo(lista_cod):\n",
    "    dados = pd.DataFrame()\n",
    "    cont = 1\n",
    "    tamanho = len(lista_cod)\n",
    "    for codigo in lista_cod: \n",
    "        if cont == 1: print(f'Total: {tamanho}. Fazendo: ')\n",
    "        print(f'{cont}, ', end = '')\n",
    "        cont+=1\n",
    "        dic = {'Código': [codigo.upper()]}\n",
    "        time.sleep(1)\n",
    "        try: # tenta a busca com símbolo zero ou símbolo vazio\n",
    "            link_codigo = link_com_codigo_BPlay(codigo)\n",
    "            dados_codigo = dados_com_link(link_codigo)\n",
    "            dic.update(dados_codigo)\n",
    "        except:\n",
    "            link_codigo = link_com_codigo_BPlay(codigo.replace('0','Ø'))\n",
    "            dados_codigo = dados_com_link(link_codigo)\n",
    "            dic.update(dados_codigo)\n",
    "        dados = pd.concat([dados, pd.DataFrame.from_dict(dic)], ignore_index = True, sort = True)\n",
    "    dados.to_excel('_Dados_vídeos_gravados2.xlsx', index = False, encoding='utf16')\n",
    "\n",
    "    \n",
    "def baixar_videos(n, nome_excel):\n",
    "    '''Baixa os n primeiros vídeos do excel gerado. (Por exemplo, ordenados por mais views no excel)\n",
    "    Maior resolução em MP4 disponível, o nome do arquivo é o código do vídeo.'''\n",
    "    \n",
    "    dados = pd.read_excel(nome_excel)\n",
    "    \n",
    "    for linha in range(n):\n",
    "        link = dados.iloc[linha,3]\n",
    "        name = dados.iloc[linha,0]\n",
    "        print(link, name)\n",
    "        YouTube(link).streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first().download(filename = name)\n",
    "    \n",
    "try:\n",
    "    df_codigos = pd.read_csv('Videos gravados.csv')\n",
    "    codigos = [cod.upper() for cod in df_codigos.iloc[:,0].tolist()]\n",
    "    #script_dados_codigo(codigos) # Chama a função para rodar o script de fato.\n",
    "except:\n",
    "    print('CSV com os códigos dos vídeos gravados não está na pasta.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
